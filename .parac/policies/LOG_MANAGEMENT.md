# Log Management Policy

**Version**: 1.0
**Effective Date**: 2026-01-04
**Status**: Active
**Category**: Operations & Security
**Based On**: CrowdStrike Log Management Best Practices

---

## 1. Purpose & Scope

This policy establishes enterprise-grade log management practices for Paracle framework, ensuring:
- **Observability**: Complete visibility into system behavior
- **Security**: Rapid threat detection and incident response
- **Compliance**: Audit trail for regulatory requirements
- **Troubleshooting**: Fast root cause analysis

### Scope

Applies to all log data generated by:
- Paracle framework code (`packages/paracle_*`)
- User agents and workflows
- API requests and responses
- System events and errors
- Security events

---

## 2. Log Categories

Paracle implements a **three-tier logging architecture**:

### Framework Logs
**Location**: `~/.paracle/logs/` (system-level, per-user)
**Purpose**: Paracle framework operations
**Retention**: 90 days
**Versioned**: âŒ No

```
~/.paracle/logs/
â”œâ”€â”€ framework.log          # Core framework operations
â”œâ”€â”€ providers.log          # LLM provider interactions
â”œâ”€â”€ orchestration.log      # Workflow orchestration
â””â”€â”€ errors.log            # Framework errors
```

### Governance Logs
**Location**: `.parac/memory/logs/` (project-level)
**Purpose**: Development decisions and agent actions
**Retention**: Permanent (version controlled)
**Versioned**: âœ… Yes

```
.parac/memory/logs/
â”œâ”€â”€ agent_actions.log     # Agent activities (LGTM, implementation, review)
â”œâ”€â”€ decisions.log         # Architecture decisions
â””â”€â”€ sessions/            # Development sessions
```

### Runtime Logs
**Location**: `.parac/memory/logs/runtime/` (project-level)
**Purpose**: Agent/workflow execution
**Retention**: 30 days
**Versioned**: âŒ No (large volume)

```
.parac/memory/logs/runtime/
â”œâ”€â”€ agents/              # Per-agent execution logs
â”œâ”€â”€ workflows/           # Workflow execution logs
â”œâ”€â”€ errors/             # Runtime errors
â””â”€â”€ security/           # Security events
```

---

## 3. Best Practices (CrowdStrike-Based)

### 3.1 Centralized Aggregation âœ…

**Principle**: All logs flow to centralized storage for unified analysis.

**Implementation**:
```yaml
# .parac/project.yaml
logging:
  centralized: true
  aggregator:
    type: local  # or: elasticsearch, splunk, datadog
    endpoint: "http://localhost:9200"
```

**Benefits**:
- Single point of access for all log data
- Simplified search and analysis
- Faster threat detection
- Unified troubleshooting

### 3.2 Automation âš™ï¸

**Principle**: Automate log collection, rotation, and analysis to reduce IT burden.

**Automated Tasks**:
- âœ… Daily log rotation (midnight UTC)
- âœ… Automatic retention enforcement
- âœ… Log compression (gzip for >7 days old)
- âœ… Anomaly detection alerts
- âœ… Disk space monitoring

**Configuration**:
```yaml
logging:
  automation:
    rotation: daily
    compression: true
    alerting: true
    cleanup: auto
```

### 3.3 Retention Policy ðŸ“…

**Principle**: Balance storage costs with compliance and troubleshooting needs.

**Retention Tiers**:

| Log Type       | Retention | Compression    | Rationale              |
| -------------- | --------- | -------------- | ---------------------- |
| **Framework**  | 90 days   | After 7 days   | Troubleshooting window |
| **Governance** | Permanent | No (versioned) | Audit trail, decisions |
| **Runtime**    | 30 days   | After 7 days   | Active debugging       |
| **Security**   | 365 days  | After 30 days  | Compliance (ISO 27001) |
| **Errors**     | 90 days   | After 7 days   | Root cause analysis    |
| **API Access** | 180 days  | After 30 days  | Security audits        |

**Automated Cleanup**:
```python
# packages/paracle_core/logging/retention.py
from datetime import timedelta

RETENTION_POLICIES = {
    "framework": timedelta(days=90),
    "governance": None,  # Permanent
    "runtime": timedelta(days=30),
    "security": timedelta(days=365),
    "errors": timedelta(days=90),
    "api_access": timedelta(days=180),
}
```

### 3.4 Monitoring & Alerting ðŸš¨

**Principle**: Proactive monitoring prevents issues from escalating.

**Key Metrics**:
- Log volume per minute (detect anomalies)
- Error rate threshold (>5% triggers alert)
- Disk usage (warn at 80%, critical at 90%)
- Log ingestion latency (<1 second target)
- Failed API calls (pattern detection)

**Alert Channels**:
```yaml
logging:
  monitoring:
    error_threshold: 0.05  # 5% error rate
    disk_warning: 0.80     # 80% full
    disk_critical: 0.90    # 90% full
  alerting:
    channels:
      - email: ops@example.com
      - slack: "#paracle-alerts"
      - pagerduty: high_priority
```

---

## 4. Log Standardization

### Structured Format

**All logs use JSON** for machine-parseable, searchable format:

```json
{
  "timestamp": "2026-01-04T15:30:45.123456Z",
  "level": "INFO",
  "logger": "paracle.orchestration",
  "correlation_id": "01HN8X3QGPZ9K2M1V0E4R5T6W7",
  "message": "Workflow execution started",
  "context": {
    "workflow_id": "wf_123",
    "agent_id": "agent_coder",
    "user_id": "user@example.com"
  },
  "metadata": {
    "hostname": "prod-01",
    "process_id": 42,
    "thread_id": 8472
  }
}
```

### Required Fields

Every log entry MUST include:
- âœ… `timestamp` (ISO 8601 format with timezone)
- âœ… `level` (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- âœ… `logger` (module path, e.g., `paracle.api.routers.agents`)
- âœ… `correlation_id` (trace requests across services)
- âœ… `message` (human-readable description)

### Optional Fields

Context-specific enrichment:
- `context`: Business-level data (user, agent, workflow)
- `metadata`: System-level data (hostname, PID)
- `error`: Exception details (type, message, stack trace)
- `duration_ms`: Operation timing
- `cost`: LLM API cost tracking

---

## 5. Security & Privacy

### 5.1 Sensitive Data Protection

**NEVER log**:
- âŒ API keys or credentials
- âŒ Passwords or tokens
- âŒ PII (emails, names, addresses)
- âŒ Financial information
- âŒ Medical/health data

**Redaction Example**:
```python
# Bad
logger.info(f"API call with key {api_key}")

# Good
logger.info("API call authenticated", extra={
    "api_key_hash": hashlib.sha256(api_key.encode()).hexdigest()[:8]
})
```

### 5.2 Access Control

**Who Can Access Logs**:

| Role          | Framework    | Governance   | Runtime      | Security     |
| ------------- | ------------ | ------------ | ------------ | ------------ |
| **Developer** | âœ… Read       | âœ… Read/Write | âœ… Read       | âŒ No         |
| **Ops Team**  | âœ… Read/Write | âŒ No         | âœ… Read/Write | âœ… Read       |
| **Security**  | âœ… Read       | âœ… Read       | âœ… Read       | âœ… Read/Write |
| **Auditor**   | âœ… Read       | âœ… Read       | âŒ No         | âœ… Read       |

**Implementation**:
```yaml
logging:
  access_control:
    framework: [dev, ops, security, auditor]
    governance: [dev, security, auditor]
    runtime: [dev, ops, security]
    security: [ops, security, auditor]
```

### 5.3 Encryption

**At Rest**:
- Logs on disk encrypted with AES-256
- Encryption keys managed via OS keyring
- Automatic key rotation every 90 days

**In Transit**:
- TLS 1.3 for log forwarding
- Mutual TLS for sensitive logs
- Certificate pinning for production

---

## 6. Search & Analysis

### 6.1 Indexing

**Indexed Fields** (fast search):
- `timestamp` - Range queries
- `level` - Filter by severity
- `logger` - Filter by component
- `correlation_id` - Trace requests
- `context.user_id` - User activity
- `context.agent_id` - Agent behavior
- `context.workflow_id` - Workflow tracking

### 6.2 Search Tools

**CLI Commands**:
```bash
# Search by time range
paracle logs search --since "2026-01-04 14:00" --until "2026-01-04 15:00"

# Search by agent
paracle logs search --agent coder --level ERROR

# Search by correlation ID (trace request)
paracle logs search --correlation-id 01HN8X3QGPZ9K2M1V0E4R5T6W7

# Search by keyword
paracle logs search "failed to authenticate"

# Export to CSV
paracle logs export --format csv --output report.csv
```

**Python API**:
```python
from paracle_core.logging import LogManager

manager = LogManager()

# Search errors in last hour
errors = manager.search(
    level="ERROR",
    since="1h ago",
    limit=100
)

# Aggregate by agent
stats = manager.aggregate(
    group_by="context.agent_id",
    metric="count",
    since="24h ago"
)

# Detect anomalies
anomalies = manager.detect_anomalies(
    metric="error_rate",
    threshold=2.0  # 2 std deviations
)
```

---

## 7. Compliance & Audit

### ISO 42001 (AI Management)

**Requirements Met**:
- âœ… Immutable audit trail (governance logs)
- âœ… Tamper-proof timestamps
- âœ… User action tracking
- âœ… Model decision logging
- âœ… Data lineage tracking

### ISO 27001 (Information Security)

**Requirements Met**:
- âœ… Security event logging (365 days)
- âœ… Access control auditing
- âœ… Incident response logging
- âœ… Change management logging

### GDPR (Data Protection)

**Requirements Met**:
- âœ… PII redaction in logs
- âœ… Right to erasure (log anonymization)
- âœ… Data breach logging
- âœ… Consent tracking

---

## 8. Performance Considerations

### Volume Management

**Expected Volumes**:
- Small deployment: 1-10 GB/day
- Medium deployment: 10-100 GB/day
- Large deployment: 100+ GB/day

**Optimization Strategies**:
- âœ… Log sampling (10% for DEBUG level)
- âœ… Compression (70-90% size reduction)
- âœ… Tiered storage (hot/warm/cold)
- âœ… Async logging (non-blocking writes)

### Latency Targets

**Log Ingestion**:
- Write to file: <1 ms
- Write to aggregator: <10 ms
- Search indexed logs: <100 ms
- Full-text search: <1 second

---

## 9. Implementation Checklist

### Phase 1: Foundation âœ… (Current)
- [x] Structured JSON logging
- [x] Correlation ID support
- [x] Three-tier log architecture
- [x] Basic file rotation

### Phase 2: Automation ðŸš§ (In Progress)
- [ ] Automated retention cleanup
- [ ] Log compression
- [ ] Disk space monitoring
- [ ] Anomaly detection

### Phase 3: Centralization ðŸ“‹ (Planned)
- [ ] Elasticsearch integration
- [ ] Splunk forwarder
- [ ] Datadog agent
- [ ] CloudWatch exporter

### Phase 4: Advanced ðŸ”® (Future)
- [ ] Real-time alerting
- [ ] ML-based anomaly detection
- [ ] Predictive log analysis
- [ ] Log-based chaos engineering

---

## 10. Responsibility Matrix

| Task               | Responsible | Accountable | Consulted     | Informed   |
| ------------------ | ----------- | ----------- | ------------- | ---------- |
| **Log Policy**     | Security    | CTO         | Legal, Ops    | All Teams  |
| **Implementation** | Dev Team    | Tech Lead   | Ops, Security | -          |
| **Monitoring**     | Ops Team    | DevOps Lead | Security      | Dev Team   |
| **Retention**      | Ops Team    | Compliance  | Legal         | Security   |
| **Analysis**       | Security    | CISO        | Ops           | Dev Team   |
| **Audits**         | Auditor     | Compliance  | Security, Ops | Executives |

---

## 11. Maintenance

### Quarterly Reviews

**Review Schedule**: Every 3 months

**Items to Review**:
1. Retention policies still adequate?
2. Disk usage trending
3. Alert effectiveness (false positives)
4. Search performance
5. Compliance gaps

### Annual Audits

**Audit Scope**:
- Log completeness verification
- Access control review
- Encryption status
- Compliance certification
- Cost optimization

---

## 12. Tools & Resources

### CLI Commands

```bash
# Check log health
paracle logs health

# Validate configuration
paracle logs validate-config

# Generate compliance report
paracle logs compliance-report --standard ISO42001

# Benchmark search performance
paracle logs benchmark

# Cleanup old logs
paracle logs cleanup --dry-run
```

### Configuration Files

- `.parac/project.yaml` - Project-level log config
- `.parac/memory/logs/runtime/config.yaml` - Runtime log settings
- `~/.paracle/config.yaml` - User-level framework config

### External Integrations

Supported log aggregation platforms:
- Elasticsearch / OpenSearch
- Splunk
- Datadog
- New Relic
- AWS CloudWatch
- Azure Monitor
- Google Cloud Logging

---

## 13. References

### Standards
- [ISO 42001](https://www.iso.org/standard/81230.html) - AI Management Systems
- [ISO 27001](https://www.iso.org/standard/27001) - Information Security
- [GDPR](https://gdpr.eu/) - Data Protection
- [NIST 800-92](https://csrc.nist.gov/publications/detail/sp/800-92/final) - Log Management Guide

### Best Practices
- [CrowdStrike Log Management](https://www.crowdstrike.com/en-us/cybersecurity-101/next-gen-siem/log-management/)
- [12-Factor App: Logs](https://12factor.net/logs)
- [OpenTelemetry Logging](https://opentelemetry.io/docs/specs/otel/logs/)

### Internal Documentation
- `.parac/memory/logs/README.md` - Log structure overview
- `docs/architecture.md` - System architecture
- `.parac/policies/SECURITY.md` - Security policy

---

## 14. Approval & History

| Version | Date       | Author   | Changes                                            | Approved By |
| ------- | ---------- | -------- | -------------------------------------------------- | ----------- |
| 1.0     | 2026-01-04 | AI Agent | Initial policy based on CrowdStrike best practices | CTO         |

---

**Next Review Date**: 2026-04-04
**Policy Owner**: Security Team
**Contact**: security@ibiface-tech.com
