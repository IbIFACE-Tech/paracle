name: Benchmarks

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save results as new baseline'
        required: false
        default: false
        type: boolean

# Ensure only one benchmark job runs at a time
concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: false

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for git info

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install uv
        uses: astral-sh/setup-uv@v1

      - name: Install dependencies
        run: |
          uv sync --all-extras

      - name: Download baseline
        uses: actions/cache@v3
        with:
          path: .benchmarks/baseline.json
          key: benchmark-baseline-${{ github.base_ref || 'main' }}
          restore-keys: |
            benchmark-baseline-main
            benchmark-baseline-

      - name: Run benchmarks
        run: |
          uv run paracle benchmark run \
            --output .benchmarks/results.json \
            --baseline .benchmarks/baseline.json \
            --verbose

      - name: Check for regressions
        if: github.event_name == 'pull_request'
        run: |
          uv run paracle benchmark run \
            --baseline .benchmarks/baseline.json \
            --fail-on-regression \
            --json-output > benchmark_report.json || exit_code=$?

          # Always save the report
          cat benchmark_report.json

          # Fail if there were regressions
          if [ "${exit_code:-0}" -ne 0 ]; then
            echo "::error::Performance regression detected!"
            exit 1
          fi

      - name: Save baseline (main branch only)
        if: |
          github.ref == 'refs/heads/main' ||
          github.event.inputs.save_baseline == 'true'
        run: |
          uv run paracle benchmark save-baseline
          echo "Baseline saved"

      - name: Update baseline cache
        if: |
          github.ref == 'refs/heads/main' ||
          github.event.inputs.save_baseline == 'true'
        uses: actions/cache@v3
        with:
          path: .benchmarks/baseline.json
          key: benchmark-baseline-main-${{ github.sha }}

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: |
            .benchmarks/results.json
            benchmark_report.json
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');

            let results;
            try {
              results = JSON.parse(fs.readFileSync('.benchmarks/results.json', 'utf8'));
            } catch (e) {
              console.log('No results file found');
              return;
            }

            const summary = results.summary;
            const benchmarks = results.results;

            let body = `## Performance Benchmark Results\n\n`;
            body += `| Benchmark | Mean (ms) | Change | Status |\n`;
            body += `|-----------|-----------|--------|--------|\n`;

            for (const b of benchmarks) {
              const change = b.change_percent ?
                `${b.change_percent > 0 ? '+' : ''}${b.change_percent.toFixed(1)}%` :
                'N/A';
              const status = {
                'passed': 'âœ…',
                'failed': 'âŒ',
                'regression': 'ðŸ”´',
                'improvement': 'ðŸŸ¢',
                'no_baseline': 'ðŸ†•'
              }[b.status] || 'â“';
              body += `| ${b.name} | ${b.mean_ms.toFixed(3)} | ${change} | ${status} |\n`;
            }

            body += `\n**Summary**: ${summary.passed} passed`;
            if (summary.regressions > 0) body += `, ${summary.regressions} regressions`;
            if (summary.improvements > 0) body += `, ${summary.improvements} improvements`;
            if (summary.failed > 0) body += `, ${summary.failed} failed`;
            body += `\n`;

            if (results.git_sha) {
              body += `\nðŸ“Š Commit: \`${results.git_sha.substring(0, 7)}\``;
            }

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('Performance Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
