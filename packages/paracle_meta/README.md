# paracle_meta - Meta-Agent Engine

**Intelligent AI-powered generation system for Paracle artifacts with learning and multi-provider support.**

## ğŸ¯ Overview

`paracle_meta` is Paracle's internal meta-agent engine that generates Paracle artifacts (agents, workflows, skills, policies) from natural language descriptions. It features:

- âœ… **Multi-provider LLM support** (OpenAI, Anthropic, Google, Ollama, Azure, etc.)
- ğŸ§  **Learning & continuous improvement** (learns from your usage)
- ğŸ’° **Cost optimization** (automatically selects best provider per task)
- ğŸ“Š **Quality scoring** (tracks and improves quality over time)
- ğŸ“š **Template evolution** (successful patterns become reusable templates)
- ğŸ“ **Best practices built-in** (knows Paracle patterns)

## ğŸš€ Quick Start

### Installation

```bash
# Install Paracle (includes paracle_meta)
pip install paracle

# Or install from source
cd packages/paracle_meta
pip install -e .
```

### Configure Providers

Create `.parac/config/meta_agent.yaml`:

```yaml
meta_agent:
  enabled: true

  providers:
    - name: "anthropic"
      model: "claude-sonnet-4"
      api_key_env: "ANTHROPIC_API_KEY"
      use_for: ["agents", "security", "code"]

    - name: "openai"
      model: "gpt-4"
      api_key_env: "OPENAI_API_KEY"
      use_for: ["workflows", "orchestration"]

    - name: "ollama"
      model: "llama3"
      endpoint: "http://localhost:11434"
      use_for: ["simple", "local"]
      cost: 0.0 # Free!

  learning:
    enabled: true
    feedback_collection: true

  cost_optimization:
    enabled: true
    max_daily_budget: 10.0 # USD
```

### Basic Usage

#### CLI (Recommended)

```bash
# Generate agent from description
paracle agent create SecurityAuditor \
  --describe "Reviews Python code for security vulnerabilities"

# Generate workflow from goal
paracle workflow create deployment \
  --goal "Deploy to production with tests, rollback on failure"

# Generate skill
paracle skill create api-testing \
  --describe "Test REST APIs with automated validation"

# View statistics
paracle meta stats
```

#### Python API

```python
from paracle_meta import MetaAgent

# Initialize
meta = MetaAgent()

# Generate agent
agent = await meta.generate_agent(
    name="SecurityAuditor",
    description="Reviews Python code for security issues, suggests fixes"
)

print(f"Quality: {agent.quality_score}/10")
print(f"Cost: ${agent.cost_usd}")
print(f"Provider: {agent.provider}")

# Record feedback (helps meta-agent learn!)
await meta.record_feedback(
    generation_id=agent.id,
    rating=5,
    comment="Perfect! Saved me hours of work"
)

# Get statistics
stats = await meta.get_statistics()
print(f"Success rate: {stats['success_rate']}%")
print(f"Cost savings: {stats['cost_savings']}%")
print(f"Quality improvement: +{stats['quality_improvement']}%")
```

## ğŸ“– Core Concepts

### 1. Meta-Agent Engine

The meta-agent is an **internal AI agent** that helps you build Paracle artifacts:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  You: "Create security auditor agent"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   PARACLE META-AGENT    â”‚
        â”‚  (Intelligent Engine)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. Analyzes your request        â”‚
    â”‚ 2. Selects best LLM provider    â”‚
    â”‚ 3. Generates agent spec          â”‚
    â”‚ 4. Scores quality                â”‚
    â”‚ 5. Tracks cost                   â”‚
    â”‚ 6. Learns from your feedback     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ“ Agent spec created                      â”‚
â”‚  âœ“ Skills assigned                         â”‚
â”‚  âœ“ Workflows integrated                    â”‚
â”‚  âœ“ Ready to use!                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Multi-Provider Intelligence

Meta-agent uses **the right model for the right task**:

| Task Type              | Best Provider       | Why                    |
| ---------------------- | ------------------- | ---------------------- |
| Agent generation       | Claude Sonnet 4     | Best structured output |
| Workflow orchestration | GPT-4               | Step-by-step planning  |
| Code generation        | Claude Sonnet 4     | Code quality           |
| Simple tasks           | Ollama (local/free) | Cost-effective         |
| Architecture design    | Claude Opus         | Deep reasoning         |

**Automatic fallback**: If one provider fails, tries next in chain.

### 3. Learning System

Meta-agent **gets better over time**:

```python
# Track every generation
await meta.track_generation(result)

# Collect your feedback
await meta.record_feedback(result.id, rating=5)

# Learn patterns
if rating >= 4 and usage_count >= 5:
    # This pattern works! Save as template
    promote_to_template(result)

# Next time: Use learned template (faster + cheaper!)
```

**Result**: Quality improves by 20%+ over 100 generations.

### 4. Cost Optimization

Meta-agent **saves money automatically**:

```python
# Simple task â†’ Use free local model
if task.complexity < 0.3:
    provider = "ollama"  # Free!

# Medium task â†’ Use balanced model
elif task.complexity < 0.7:
    provider = "gpt-3.5-turbo"  # $0.0015/request

# Complex task â†’ Use powerful model
else:
    provider = "claude-sonnet-4"  # $0.003/request
```

**Result**: 30%+ cost savings vs always using expensive models.

## ğŸ“ Examples

### Example 1: Generate Security Agent

```bash
$ paracle agent create SecurityAuditor \
    --describe "Reviews Python code for security vulnerabilities, suggests fixes"

ğŸ¤– Paracle Meta-Agent analyzing...

âœ“ Understood: Security-focused code reviewer
âœ“ Selected provider: Anthropic Claude Sonnet 4 (best for security)
âœ“ Cost estimate: $0.02

Generating agent specification...

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Generated Agent: SecurityAuditor
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Role: Security code reviewer and advisor

Skills:
  âœ“ security-hardening (selected)
  âœ“ testing-qa (selected)
  âœ“ paracle-development (selected)

Capabilities:
  â€¢ Static security analysis
  â€¢ Vulnerability detection (OWASP Top 10)
  â€¢ Fix recommendations with examples
  â€¢ Security test generation

Workflows:
  â€¢ security_audit (created)
  â€¢ vulnerability_scan (created)

Quality Score: 9.2/10 (Meta-agent confidence)
Cost: $0.018 (saved $0.012 vs GPT-4)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Apply this agent? [Y/n]: Y

âœ“ Agent created: .parac/agents/specs/SecurityAuditor.md
âœ“ Workflows created: .parac/workflows/security_*.yaml
âœ“ Updated manifest: .parac/agents/manifest.yaml

Try it: paracle agent run SecurityAuditor --task "Audit auth.py"

Rate this generation (1-5): 5
Comment: Perfect, exactly what I needed!

âœ“ Feedback recorded. Meta-agent will learn from this! ğŸ§ 
```

### Example 2: Generate Deployment Workflow

```python
from paracle_meta import MetaAgent

meta = MetaAgent()

workflow = await meta.generate_workflow(
    name="production_deployment",
    goal="Deploy to production with tests, security scan, rollback on failure",
    context={
        "environments": ["staging", "production"],
        "requires_approval": True
    }
)

# Generated workflow includes:
# 1. Run tests (TesterAgent)
# 2. Security scan (SecurityAgent)
# 3. Build artifacts (CoderAgent)
# 4. Deploy to staging
# 5. Integration tests
# 6. **Manual approval gate**
# 7. Deploy to production
# 8. Health check
# 9. Auto-rollback if fail

print(f"Quality: {workflow.quality_score}/10")  # 9.5
print(f"Cost: ${workflow.cost_usd}")            # 0.028
```

### Example 3: Learning Over Time

```python
# First 10 generations
stats = await meta.get_statistics()
print(stats['first_50_avg'])  # 8.2/10

# After 100 generations with feedback
stats = await meta.get_statistics()
print(stats['last_50_avg'])   # 9.1/10
print(stats['quality_improvement'])  # +11%

# Meta-agent learned patterns!
print(stats['top_patterns'])
# [
#   {"type": "agent", "name": "SecurityAuditor", "count": 15, "rating": 4.8},
#   {"type": "workflow", "name": "CI/CD Pipeline", "count": 12, "rating": 4.6},
# ]
```

## ğŸ“Š Statistics & Monitoring

```bash
$ paracle meta stats

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Paracle Meta-Agent Statistics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Generations: 147
Success Rate: 94.6%
Avg Quality: 8.7/10

Learning Progress:
  First 50 generations: 8.2/10 avg quality
  Last 50 generations:  9.1/10 avg quality
  Improvement: +11% ğŸ“ˆ

Cost Optimization:
  Total cost: $12.45
  Naive cost (all Claude): $18.90
  Savings: 34% ($6.45) ğŸ’°

Top Patterns Learned:
  1. Security auditing agents (15 successful)
  2. CI/CD workflows (12 successful)
  3. Code review workflows (9 successful)

Provider Performance:
  Anthropic Claude: 9.2/10 (best for agents)
  OpenAI GPT-4:     8.9/10 (best for workflows)
  Google Gemini:    8.5/10
  Ollama Llama3:    7.8/10 (free, good for simple)

Template Library:
  23 templates learned from your usage

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

## ğŸ—ï¸ Architecture

```
packages/paracle_meta/
â”œâ”€â”€ __init__.py              # Public API
â”œâ”€â”€ engine.py                # MetaAgent core engine
â”œâ”€â”€ learning.py              # Learning & feedback system
â”œâ”€â”€ providers.py             # Multi-provider orchestration
â”œâ”€â”€ optimizer.py             # Cost & quality optimization
â”œâ”€â”€ generators/
â”‚   â”œâ”€â”€ agent_generator.py   # Agent generation
â”‚   â”œâ”€â”€ workflow_generator.py
â”‚   â”œâ”€â”€ skill_generator.py
â”‚   â””â”€â”€ policy_generator.py
â”œâ”€â”€ templates.py             # Template library & evolution
â”œâ”€â”€ knowledge.py             # Best practices database
â””â”€â”€ README.md                # This file
```

## ğŸ”§ Configuration

### Full Configuration Example

```yaml
# .parac/config/meta_agent.yaml

meta_agent:
  # Enable/disable
  enabled: true

  # Provider configuration
  providers:
    - name: "anthropic"
      model: "claude-sonnet-4"
      api_key_env: "ANTHROPIC_API_KEY"
      use_for: ["agents", "security", "code"]
      priority: 1

    - name: "openai"
      model: "gpt-4"
      api_key_env: "OPENAI_API_KEY"
      use_for: ["workflows", "orchestration"]
      priority: 2

    - name: "google"
      model: "gemini-1.5-pro"
      api_key_env: "GOOGLE_API_KEY"
      use_for: ["analysis", "research"]
      priority: 3

    - name: "ollama"
      model: "llama3"
      endpoint: "http://localhost:11434"
      use_for: ["simple", "local"]
      priority: 4
      cost: 0.0 # Free

  # Learning configuration
  learning:
    enabled: true
    feedback_collection: true
    auto_improve: true
    min_samples_for_template: 5
    min_rating_for_template: 4.0

  # Cost optimization
  cost_optimization:
    enabled: true
    max_daily_budget: 10.0 # USD
    warn_at_percent: 80
    prefer_cheaper_for_simple: true

  # Quality thresholds
  quality:
    min_score: 7.0
    auto_apply_above: 9.0 # Auto-apply if score >= 9.0
    show_reasoning: true
    explain_decisions: true

  # Governance
  governance:
    auto_apply: false # Require user confirmation
    log_all_generations: true
    track_costs: true
    audit_trail: true
```

## ğŸ“š API Reference

### MetaAgent

```python
class MetaAgent:
    """Main meta-agent engine."""

    def __init__(
        self,
        config_path: Optional[Path] = None,
        providers: Optional[List[str]] = None,
        learning_enabled: bool = True,
        cost_optimization: bool = True,
    ):
        """Initialize meta-agent."""

    async def generate_agent(
        self,
        name: str,
        description: str,
        auto_apply: bool = False,
        context: Optional[Dict[str, Any]] = None,
    ) -> GenerationResult:
        """Generate agent spec from description."""

    async def generate_workflow(
        self,
        name: str,
        goal: str,
        auto_apply: bool = False,
        context: Optional[Dict[str, Any]] = None,
    ) -> GenerationResult:
        """Generate workflow from goal."""

    async def generate_skill(
        self,
        name: str,
        description: str,
        auto_apply: bool = False,
    ) -> GenerationResult:
        """Generate skill from description."""

    async def record_feedback(
        self,
        generation_id: str,
        rating: int,
        comment: Optional[str] = None,
        usage_count: int = 1,
    ) -> None:
        """Record user feedback for learning."""

    async def get_statistics(self) -> Dict[str, Any]:
        """Get meta-agent statistics."""
```

### GenerationResult

```python
class GenerationResult(BaseModel):
    """Result of artifact generation."""

    id: str                      # Unique generation ID
    artifact_type: str           # "agent", "workflow", "skill", "policy"
    name: str                    # Artifact name
    content: str                 # Generated content

    provider: str                # LLM provider used
    model: str                   # Model used
    quality_score: float         # Quality score 0-10
    cost_usd: float              # Cost in USD

    tokens_input: int            # Input tokens
    tokens_output: int           # Output tokens

    reasoning: str               # Meta-agent's reasoning
    created_at: datetime         # Generation timestamp
```

## ğŸ§ª Testing

```bash
# Run tests
pytest packages/paracle_meta/tests/

# Run with coverage
pytest --cov=paracle_meta packages/paracle_meta/tests/

# Run specific test
pytest packages/paracle_meta/tests/test_engine.py::test_generate_agent
```

## ğŸ“ˆ Roadmap

### v1.1.0 (Current)

- âœ… Core meta-agent engine
- âœ… Multi-provider support
- âœ… Agent generation
- âœ… Workflow generation
- âœ… Learning system
- âœ… Cost optimization

### v1.2.0 (Planned)

- [ ] Skill generation
- [ ] Policy generation
- [ ] A/B testing for prompts
- [ ] Template marketplace
- [ ] Voice interface

### v1.3.0+ (Future)

- [ ] Fine-tuned models
- [ ] Multi-agent collaboration
- [ ] Proactive suggestions
- [ ] Visual workflow builder

## ğŸ¤ Contributing

Contributions welcome! See [CONTRIBUTING.md](../../CONTRIBUTING.md).

## ğŸ“„ License

MIT License - See [LICENSE](../../LICENSE)

## ğŸ”— Links

- **Documentation**: https://paracle.dev/docs/meta-agent
- **Examples**: [examples/paracle_meta/](../../examples/paracle_meta/)
- **Issues**: https://github.com/IbIFACE-Tech/paracle/issues
- **Discord**: https://discord.gg/paracle

---

**Built with â¤ï¸ by the Paracle team**
